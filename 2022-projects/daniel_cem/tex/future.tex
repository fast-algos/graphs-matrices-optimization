\section{Limitations and Future Works}

\paragraph{Unifying regularization-based approaches.}
We have introduced four categories of regularization-based approaches, and each has its own merits and demerits. 
For deterministic approaches, regularizing in the weight-space only needs to store a matrix or vector penalizing different weights, but the regularizations are usually not meaningful anymore when the weights are far from the old weights $\bw_0$. In comparison, the function-space provides a universally meaningful regularization but requires to store representative coresets of old tasks. On the other hand, the KL regularization promotes online posterior inference in both the weight-space and the function-space, but it might be difficult to approximate the true posterior accurately in each task and the approximation error might accumulate. Therefore, we need to potentially combine these approaches in a unified model that enjoys all their benefits. 

\paragraph{Selecting the coreset.} Function-space approaches \citep{benjamin2018measuring, buzzega2020dark, rebuffi2017icarl} usually require to store a small \emph{coreset} that is representative of the dataset. Moreover, the coreset is also necessary in lazy inference \citep{nguyen2017variational} and gradient episodic memory \citep{lopez2017gradient, chaudhry2018efficient}. Because of the memory budget, selecting the coreset highly impacts the continual learning performance. Reservoir sampling has been proposed to uniformly sample the coreset from the data stream \citep{buzzega2020dark, chrysakis2020online}. Intuitively, the coreset should contain diverse examples, thus \citet{aljundi2019gradient, titsias2019functional} propose to choose the coreset by maximizing a certain dissimilarity metric. \citet{borsos2020coresets} also proposes to choose a weighted coreset by matching its maximum-likelihood-estimator (MLE) to that of the whole dataset \citep{campbell2019automated}. In general, how to select the coreset and how to use the coreset still require more attention in the future research.

\paragraph{Other data pipelines.} In this paper we mainly focus on the simplest continual learning data pipeline, where the task identifiers are known and the dataset in each task can be visited multiple times. If the task identifiers are unknown, then detecting the task boundaries \citep{titsias2019functional} or minimizing the dependencies of the task identifiers will be necessary for the learner to work well. Furthermore, if the training data comes in a stream, the learner needs efficient online algorithms to learn from the new task and to remember the old tasks. For example, the reservoir sampling \citep{buzzega2020dark} has been proposed to choose the coreset online while making sure it is uniformly sampling from the stream.

\paragraph{The task identifiers.} Although an artificial general intelligence should learn well even if the streaming tasks are totally unrelated, having the structural similarities between tasks is more appealing in continual learning problems. The structural similarities can be reflected by the task identifiers, such as a natural language describing the task \citep{chaudhry2018efficient} or an image representative of the present vision dataset. This setting is also relevant to the few-shot learning problems \citep{snell2017prototypical} where the learner is expected to learn quickly from a small set of examples. However, current continual learning benchmarks usually not contain structural identifiers but only an integer representing the index of the task. In consequence, most continual learning approaches focus more on preventing the catastrophic forgetting but neglect the forward transfer. Therefore, it will be valuable to have continual learning benchmarks that can take use of the structural identifiers and potentially enable another flourishment of continual learning approaches.

 
\section{Conclusion}
In this paper I introduce a category for regularization-based approaches and study the connections between them. My analyses provide a general perspective about how to tackle the continual learning problem and learn without the catastrophic forgetting issue. My analyses are accompanied with the benefits and limitations of current methods, based on which I bring up important directions for future continual learning research.