<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>prelims</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="prelims_files/libs/clipboard/clipboard.min.js"></script>
<script src="prelims_files/libs/quarto-html/quarto.js"></script>
<script src="prelims_files/libs/quarto-html/popper.min.js"></script>
<script src="prelims_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="prelims_files/libs/quarto-html/anchor.min.js"></script>
<link href="prelims_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="prelims_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="prelims_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="prelims_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="prelims_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<p>Here we will cover some preliminaries on spectral sparsification and then we will discuss the effective resistance-based algorithm for spectral sparsification. We will also discuss an important reduction to the matrix problem which lays the groundwork for the final algorithm.</p>
<section id="spectral-sparsification" class="level3">
<h3 class="anchored" data-anchor-id="spectral-sparsification">Spectral Sparsification</h3>
<p>Before everything, we should define what a spectral sparsifier is. A spectral sparsifier is a sparse graph that approximates the Laplacian of a graph with high accuracy. In other words, a sparsifier is a graph that has a lot of the same properties as the original graph; formally,</p>
<div id="def-spectral-sparsification" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 </strong></span>A <span class="math inline">\((k, \epsilon)\)</span>-spectral sparsifier of a graph <span class="math inline">\(G = (V, E, w)\)</span> is a graph <span class="math inline">\(H\)</span> with <span class="math inline">\(k\)</span> edges such that, <span class="math display">\[L_G \approx_\epsilon L_H : (1 - \epsilon) L_G \preceq L_H \preceq (1 + \epsilon) L_G\]</span> where <span class="math inline">\(L_G\)</span> is the Laplacian of <span class="math inline">\(G\)</span> and <span class="math inline">\(L_H\)</span> is the Laplacian of <span class="math inline">\(H\)</span>.</p>
</div>
</section>
<section id="reduction-to-the-matrix-problem" class="level3">
<h3 class="anchored" data-anchor-id="reduction-to-the-matrix-problem">Reduction to the Matrix Problem</h3>
<p>Here, we will present an analog problem for the sparsification of matrices that is tightly connected to the spectral sparsification problem. The problem is as follows:</p>
<div id="def-matrix-approximation" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 </strong></span><strong><span class="math inline">\((k, \epsilon)\)</span>-approximation of matrices</strong> Given a set of <span class="math inline">\(m\)</span> vectors <span class="math inline">\(v_1, \ldots, v_m \in \mathbb{R}^n\)</span> if <span class="math inline">\(A = \sum_{i=1}^m v_iv_i^T\)</span> is a positive semi-definite matrix, then we intend to find a subset of vectors <span class="math inline">\(\mathcal{S} \subseteq \{1, \ldots, m\}\)</span> of size <span class="math inline">\(k\)</span> and a set of coefficients <span class="math inline">\(s_i \in \mathbb{R}^+\)</span> such that <span class="math inline">\(\hat{A} = \sum_{i \in \mathcal{S}} s_i \cdot v_i v_i^T\)</span> and <span class="math inline">\(A \approx_\epsilon \hat{A}\)</span>.</p>
</div>
<p>Now we will show that one can solve the <span class="math inline">\((k, \epsilon)\)</span> problem in <a href="#def-matrix-approximation">Definition&nbsp;2</a> then plug it into the graph sparsification problem and obtain a <span class="math inline">\((k, \epsilon)\)</span>-spectral sparsifier. To do so, observe that if we set <span class="math inline">\(A = L_G\)</span> and <span class="math inline">\(v_{ab} = \sqrt{w_G(a,b)} (\chi_a - \chi_b)\)</span> and <span class="math inline">\(s_{ab} = \frac{w_H(a,b)}{w_G(a,b)}\)</span>, then the problem in <a href="#def-matrix-approximation">Definition&nbsp;2</a> is equivalent to the spectral sparsification problem:</p>
<p><span class="math display">\[\begin{align*}
A = L_G &amp;= \sum_{(a,b) \in E(G)} w_G(a,b) L_{ab} \\
&amp;= \sum_{(a,b) \in E(G)} \sqrt{w_G(a,b)}^2 (\chi_a - \chi_b) (\chi_a - \chi_b)^T\\
&amp; = \sum_{ab \in E(G)} v_{ab} v_{ab}^T\\
\hat{A} = L_H &amp;= \sum_{(a, b) \in E(H)} w_H(a,b) L_{ab} \\
&amp;= \sum_{(a, b) \in E(H)} \frac{w_H(a,b)}{w_G(a,b)} \sqrt{w_G(a,b)}^2 (\chi_a - \chi_b) (\chi_a - \chi_b)^T\\
&amp;= \sum_{(a,b) \in E(H)} s_{ab} v_{ab} v_{ab}^T
\end{align*}\]</span></p>
</section>
<section id="sampling-based-sparsification" class="level3">
<h3 class="anchored" data-anchor-id="sampling-based-sparsification">Sampling-based Sparsification</h3>
<p>As alluded to previously, the problem of spectral sparsification can be approached from an edge-sampling perspective. In particular, one can assign importance weights to each edge and then come up with a sampling scheme that samples edges according to their importance. For example, an edge that is crucial for the connectivity of the graph has high importance for cut-sparsifiers or spectral sparsifiers. To that end, a set of edges can be independently sampled according to this scheme and after sampling each edge the graph becomes more and more similar to the original graph. However, since this sampling is done according to the measure of importance, even after sampling a small number of edges, the graph becomes a good approximation of the original graph.</p>
<p>One can also formulate the same thing for the matrix approximation problem. Assume that for each vector <span class="math inline">\(i\)</span>, we have a corresponding matrix <span class="math inline">\(X_i = s_i v_i v_i^T\)</span> which will be picked with probability <span class="math inline">\(p_i\)</span> and we will consider <span class="math inline">\(\hat{A} = \sum_{i \in \mathcal{S}} X_i\)</span> where <span class="math inline">\(\mathcal{S}\)</span> is the set of indices of the sampled vectors. One can bound the number of sampled vectors by coming up with good probabilities <span class="math inline">\(p_i\)</span> such that <span class="math inline">\(\sum p_i\)</span> is bounded by <span class="math inline">\(k\)</span> and can also bound the error of the approximation by using matrix concentration bounds. However, these algorithms tend to have the following problems:</p>
<ol type="1">
<li>The algorithm is not deterministic meaning that there is a very low chance of producing a large set <span class="math inline">\(\mathcal{S}\)</span>.</li>
<li>The algorithm is not deterministic meaning that there is a very low chance of producing an approximate <span class="math inline">\(\hat{A}\)</span> which is not close to <span class="math inline">\(A\)</span>.</li>
<li>Because these algorithms rely on exponential concentration bounds, typically they require to sample <span class="math inline">\(\mathcal{O}(n \cdot polylog(n))\)</span> vectors to achieve a good approximation.</li>
</ol>
<p>Although flawed, these solutions are easy and using this idea, a set of sampling techniques have been proposed to tackle the problem of sparsification with the most famous among them being the <strong>effective-resistance</strong> based sparsifiers <span class="citation" data-cites="spielman2008graph">[@spielman2008graph]</span>. We will briefly cover the main idea and intuition behind this and redirect the reader to other resources for further detailed reading.</p>
<p>The effective resistance between two nodes <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is the equivalent resistance if we assume that the rest of the nodes are harmonic and only one external current is given to <span class="math inline">\(a\)</span> and one external current is taken from <span class="math inline">\(b\)</span>; then, the measured voltage difference between these two nodes will denote the effective resistance which can be written as <span class="math inline">\((\chi_a - \chi_b)^T L^+_G (\chi_a - \chi_b)\)</span> using Laplacians. Moreover, effective resistances have a combinatorial interpretation as well. If we assume we sample spanning trees proportional to their weight products, then the effective resistance between two nodes is proportional to the probability of the edge between those two nodes appearing. This means that a crucial edge in the connectivity, will have a high probability of appearing in the sampled spanning trees and thus will have a high effective resistance; that said, this will yield a high importance weight for that edge and thus it will be sampled more often:</p>
<p><strong>Effective-resistance based sparsifier</strong> For each edge <span class="math inline">\((a, b) \in E\)</span>, sample <span class="math inline">\((a,b)\)</span> with probability <span class="math inline">\(p(a,b) = \min\left(1, C\cdot (\log n) \epsilon^{-2} w(a,b) R_{eff}(a, b)\right)\)</span>. Where <span class="math inline">\(R_{eff}(a, b)\)</span> is the effective resistance between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Using Rudelson concentration lemma <span class="citation" data-cites="rudelson1999random">[@rudelson1999random]</span>, <span class="citation" data-cites="spielman2008graph">[@spielman2008graph]</span> shows that for a certain constant <span class="math inline">\(C\)</span> after picking <span class="math inline">\(\mathcal{O}(n\log n /\epsilon)\)</span> edges the resulting graph is a <span class="math inline">\(\epsilon\)</span>-spectral sparsifier with high probability.</p>
<!-- To show that, we use the following concentration-bound theorem:

::: {#thm-rudelson}
Let $X_1, \ldots, X_n$ be independent random positive semidefinite matrices such that $||X_i|| \le R$ almost surely. Let $X = \sum X_i$ and let $\mu_{\min}$ and $\mu_{\max}$ be the minimum and maximum eigenvalues of $E[X]$. Then, for $1 > \epsilon > 0$,
$$Pr\left[ \lambda_{\min} (\sum X_i) \le (1 - \epsilon) \mu_{\min} \right] \le n \left( \frac{e^{-\epsilon}}{(1 - \epsilon)^{(1 - \epsilon)}} \right)^{\mu_{\min}/R} \le e^{-\epsilon^2 / 2},$$
and for $\epsilon > 0$,
$$Pr\left[ \lambda_{\max} (\sum X_i) \le (1 + \epsilon) \mu_{\max} \right] \le n \left( \frac{e^{-\epsilon}}{(1 + \epsilon)^{(1 + \epsilon)}} \right)^{\mu_{\max}/R} \le e^{-\epsilon^2 / 3}.$$
:::

Now if we return to the idempotent matrix $\Pi$ in the previous section and plug in the Laplacians in the formula we obtain $\Pi = L_G^{+/2} L_G L_G^{+/2}$. Then, approximating $\Pi$ using $\hat{\Pi}$ will yield a graph that is a spectral sparsifier.
Now the idea is to assign a random positive semi-definite matrix to each edge $(a, b)$ and pick that matrix with a probability $p(a,b)$ which will give us a set of random matrices $X_{ab}$ that sum up to $\hat{\Pi}$. By applying a simple union bound we can show that:
$$Pr[\hat{\Pi} \approx_\epsilon \Pi] \ge 1 - Pr[\hat{\Pi} \succ (1 + \epsilon) \Pi]- Pr[\hat{\Pi} \prec (1 - \epsilon) \Pi]$${#eq-randomized-sparsification}
Hence, since $\Pi$ has only eigenvalues equal to one or zero, then we can apply @thm-rudelson with $\mu_{\min} = 1$ and $\mu_{\max} = 1$ because we only consider the space orthogonal to the kernel. Then we can show that both the values in the right-hand side of @eq-randomized-sparsification are at most $n 
\cdot e^{-\epsilon^2 / 3R}$ if the lemma conditions hold. We will indeed show that for $X_{ab} = w(a,b) / p(a,b) \cdot L_G^{+/2} L_{ab} L_G^{+/2}$, this is the case:

1. $\forall a, b \in V: ||X_{ab}|| \le R$

    Note that for $R = \frac{1}{C \log n \epsilon^{-2}}$ we can prove this:
    \begin{align*}
    ||X_{ab}|| &\le Tr(X_{ab}) = w(a,b) / p(a,b) \cdot Tr(L_G^{+/2} L_{ab} L_G^{+/2}) \\ 
    &\le w(a,b) / p(a,b) \cdot Tr(L_G^{+/2} (\chi_a - \chi_b) (\chi_a - \chi_b)^T L_G^{+/2}) \\ 
    & = w(a,b) / p(a,b) \cdot Tr((\chi_a - \chi_b)^T L_G^+ (\chi_a - \chi_b)) \\
    & = w(a,b) / p(a,b) \cdot R_{eff}(a, b) \le \frac{1}{C (\log n) \epsilon^{-2}}\\
    \end{align*}
2. $E[\sum_{a,b} X_{ab}] = \Pi$


    This can be easily shown by plugging in the formula:
    \begin{align*}
    E[\sum_{a,b} X_{ab}] &= \sum_{a,b} E[X_{ab}] = \sum_{a,b} p(a,b) \frac{w(a,b)}{p(a,b)} \cdot L_G^{+/2} L_{ab} L_G^{+/2}\\
    & = L_G^{+/2} \left( \sum_{a,b} w(a,b) L_{ab} \right) L_G^{+/2} = L_G^{+/2} L_{G} L_G^{+/2} = \Pi
    \end{align*}




::: {#cor-randomized-sparsification}
::: -->
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>